<<<<<<< Updated upstream
---
title: "Projet sujet 2"
author: "Omar Himych, Morgane Roy"
date: "03/03/2022"
output:
  html_document:
    toc: yes
    toc_depth: 4
    number_sections: yes
---

```{r setup, include = FALSE}
# Setup of options for the rmarkdown document
knitr::opts_chunk$set(echo = TRUE, # By default, codes are shown
                      warning = FALSE, # Warnings are not printed
                      message = FALSE, # Messages are not printed
                      cache = TRUE,
                      out.width = "90%",
                      fig.align = 'center') # Codes results are cached into a 
# dedicated _cache directory which should be ignored by git (and emptied regularly)
```

```{r work, include =FALSE, cache=FALSE}
# set environment
WORK = "~/3A_202120222/Valeurs extrêmes/Projet/" #à modifier en local pour chacun
setwd(WORK)

library(dplyr)
library(ggplot2)
library(lubridate)
library(extRemes)
library(evd)

data <- load("data.RData")

data_df <<- select(data_df, "date", "var2") 
data2_df <<- select(data2_df, "date", "var3", "var4")

data_df_plot <- data_df

class(data_df$DATE)
#transform to appropriate time class:
data_df$date = as.POSIXlt(data_df$date)
years = 1900 + data_df$date$year
months = data_df$date$mon+1
table(months)
data_df$years = years
data_df$months = months

```
# Introduction

Dans le cadre du projet de valeurs extrêmes, nous avons eu le projet N°2. Pour les tâches 1 à 5 nous avons donc travaillé avec var2 pour *data_df*, et pour la suite, nous avons travaillé avec var3 et var4 pour *data2_df*.

***

# Tâche 1

Pour commencer, voici d'abord un résumé de nos données ainsi qu'un tracé temporel de toutes les données.
```{r}

summary(data_df)

ggplot(data = data_df_plot,
       mapping = aes(x = date, y = var2)) + geom_line() + labs(
  title    = "Évolution temporelle de la variable étudiée",
  subtitle = "Tracé sur la totalité des dates fournies, début des mesures en 1971",
  x        = "Date",
  y        = "Variable")

# format date données : YYYY-MM-JJ

date_debut <- as.Date("2010-01-01","%Y-%m-%j")
date_fin <- as.Date("2019-12-31", "%Y-%m-%j")

ggplot(data = data_df_plot %>% filter(between(as.Date(date,origin="1970-01-01"), date_debut,date_fin)),
       mapping = aes(x = date, y = var2)) + geom_line() + labs(
  title    = "Évolution temporelle de la variable étudiée",
  subtitle = "Période restreinte aux années 2010",
  x        = "Date",
  y        = "Variable")

date_debut <- as.Date("2018-01-01","%Y-%m-%j")
date_fin <- as.Date("2018-12-31", "%Y-%m-%j")
ggplot(data = data_df_plot %>% filter(between(as.Date(date,origin="1970-01-01"), date_debut,date_fin)),
       mapping = aes(x = date, y = var2)) + geom_line()  + labs(
  title    = "Évolution temporelle de la variable étudiée",
  subtitle = "Tracé sur l'année 2018",
  x        = "Date",
  y        = "Variable")

hist(na.omit(data_df$var2), breaks = 15, main = "Histogramme de la variable", xlab = "var2", ylab="Fréquence", freq=F)
lines(density(na.omit(data_df$var2)), lwd=2, col='red')


acf(data_df$var2, na.action = na.pass, xlab = "Lag (jours)", ylab="Auto-corrélation temporelle de var2", main="Auto-corrélation temporelle de la variable en fonction du lag")


pacf(data_df$var2, na.action = na.pass, xlab = "Lag (jours)", ylab="Auto-corrélation temporelle de var2", main="Auto-corrélation temporelle de la variable en fonction du lag")


```

Nous pouvons constater qu'il s'agit d'une variable avec une certaine saisonnalité, et qui semble atteindre ces valeurs les plus extrêmes en été. Tout cela, en plus de l'échelle de valeurs (de ~7 à ~37) laisse penser à une température. Il s'agit également d'une variable ayant une majorité de valeurs vers 25/26, donc plutôt une région chaude, et qui est auto-corrélée dans le temps de manière assez significative.

# Tâche 2

```` {r, fig.align='center'}

extract_function = function(vec, nr_of_na_allowed = 30){
  if(sum(is.na(vec)) > nr_of_na_allowed){
    return(NA)
  }else{
    return(max(vec, na.rm = TRUE))
  }
}


tmp = aggregate(data_df$var2, by = list(year = data_df$years), FUN = extract_function)
mean(is.na(tmp$x)) # proportion of years with too many NA data
# observations of maxima
obs = tmp$x

ggplot(data = tmp,
       mapping = aes(x = year, y = x)) + geom_line()  + labs(
  title    = "Maxima annuel sur les années avec moins de 30 NA",
  x        = "Year",
  y        = "Maximum")


````

On constate une hausse des maxima annuels sur les années avec moins de 30 données NA.
Faisons maintenant l'estimation du modèle GEV stationnaire puis du modèle non-stationnaire.

```` {r, fig.align='center', warning=FALSE}

fit = fevd(na.omit(obs), type = "GEV", method = "MLE", period.basis = "year", use.phi = TRUE)
summary(fit)
plot(fit, type="density", main="")

fit_nonstationnaire1 = fevd(na.omit(obs), data = data.frame(year = na.omit(tmp)$year), scale.fun = ~1+year, location.fun = ~1+year, type = "GEV", method = "MLE", period.basis = "year", use.phi = TRUE)
summary(fit_nonstationnaire1) 
plot(fit_nonstationnaire1, type="density", main="")

````

Le modèle non-stationnaire a un output d'AIC et de BIC plus faibles, ce qui indique qu'il s'agit d'un modèle qui correspond mieux. On le voit également avec le graphique de densité, où le modèle correspond beaucoup mieux aux données dans le cas du modèle non-stationnaire.

Comme nous avons pu constater que les maxima annuels ont récemment augmenté, ce qui peut être expliqué par le changement climatique dans le cas d'une températuer - notre supposition pour notre variable, il n'est pas étonnant que le modèle non-stationnaire soit plus correct.


Calculons maintenant le 100-year return pour le modèle stationnaire et le modèle non-stationnaire pour 2020.
````{r, eval=FALSE}
# Estimate a 100-year return level for var 2 for the year 2020. 
T_2020 = 366 # in 100 years, we have T days in January


covar1 = cos(1:366/366*2*pi)
covar2 = sin(1:366/366*2*pi)
# For the shape parameter, we simply use the average of estimates for the month of January:
sigma_jan = mean(exp(sigma0 + covar1[1:31] + sigma2 * covar2[1:31]))
sigma_jan
xi = fit$results$par[4]
xi
names(xi) = NULL
u = u_by_month[1]  #threshold in January month

# By construction of the monthly thresholds above, the exceedance probability is as follows:
p_u = 1 - 0.975
x_T = u + sigma_jan * ((T_january*p_u)^xi-1)/xi
x_T 


plot(fit_nonstationnaire1, type="rl",main="", rperiods=100)


````

````{r}


````
# Tâche 3

- Réponse question (a)
 
Cette fois, nous modélisons les valeurs extrêmes da la variable 2 en utilisant l'approche Peaks-over-Threshold. Premièrement, nous allons  déterminer le seuil approprié *u*, par la méthode graphique de l'excès moyen.


### Mean excess plot

Le graphe d'excès moyen est un outil visuel permettant de choisir un seuil convenable qui correspondand à un comportement linéaire de l'excès moyen. Ce résultat est basé sur la propriété de stabilité du seuil du GPD.

```{r mean plot}

evd::mrlplot(na.omit(data_df$var2), tlim = c(quantile(na.omit(data_df$var2), .5), quantile(na.omit(data_df$var2), .995)))
abline(v=29.45,col='red') #seuil
abline(h=1,col='blue') # ordonnée correspondante

```

D'après le resultat précedent, nous pouvons estimer le seuil à u=29.45 puisque le comportement se stabilise approximativement à partir de cette valeur.

- Réponse question (b)

Nous sommes toujours dans le cadre univarié (variable 2), mais avec l'hypothèse des observations non-indépendantes. L'autocorrélation caractérise les probabilités de co-occurrence de valeurs élevée au décalage temporel h.


```{r tail autocorrelation function}

prob = .9 # probabilité pour le quantile à utiliser comme seuil
tmp = atdf(as.ts(na.omit(data_df$var2)), u = prob, plot = FALSE, lag.max = 50, type = "rho")
par(mar = c(5, 5, .5, .5), cex.lab = 2, cex.axis = 2, lwd = 2, cex = 1)
plot(tmp, main = "", xlab = "h", ylab = expression(chi(u,h)))
lines(c(0, 100), rep(1-prob,2), lwd = 2, lty = 2, col = "red")

```

Nous pouvons constater que la dépendance temporelle extrême semble assez forte sur des décalages relativement importants.

Ensuite, on va essayer d'augmenter le seuil.

```{r tail autocorrelation function (cas 2)}

prob = .995 # augmenter le seuil
tmp = atdf(as.ts(na.omit(data_df$var2)), u = prob, plot = FALSE, lag.max = 50, type = "rho")
par(mar = c(5, 5, .5, .5), cex.lab = 2, cex.axis = 2, lwd = 2, cex = 1)
plot(tmp, main = "", xlab = "h", ylab = expression(chi(u,h)))
lines(c(0, 100), rep(1-prob,2), lwd = 2, lty = 2, col = "red")

```

La dépendance temporelle semble diminuer lorsqu'on fixe seuil plus élevé. Nous pouvons améliorer encore ce résultat en considérant le cas non-stationnaire et filtrer les données par mois ou par saison.


- Réponse question (c)

Nous allons regouper les valeurs qui dépassent le seuil dans des clusters d'une longeur d'intervalle égale à 1.

```{r dégroupage des dépassements de seuil}

cl = clusters(data_df$var2, u =quantile(na.omit(data_df$var2), 0.995) , r = 1)

length(cl) # nombre des groupes(clusters)

cl[[21]]

```

Donc, le nombre des clusters est : 22.

```{r plot clusters}
clusters(data_df$var2, u =quantile(na.omit(data_df$var2), 0.995) , r = 1, plot = TRUE, col = "red", lvals = FALSE)

```

### GPD model estimation 


- Réponse question (d)

Nous estimons le modèle GPD pour la variable 2:

Alors, nous allons déterminer le seuil analytiquement par la fonction quantile à 90% :

```{r}
u = quantile(na.omit(data_df$var2), 0.995)
u # le seuil...
```

Donc, la valeur obtenue est proche de celle extraite graphiquement (environ 30).


```{r estimation de GPD}
sum(na.omit(data_df$var2)> u, na.rm = TRUE) # nombre de dépassements

fit2 = fevd(x = na.omit(data_df$var2),threshold = u, type = "GP", method = "MLE", use.phi = TRUE)
summary(fit2)

plot(fit2)

# la densité de notre modèle:

plot(fit2, type = "density", main = "Empirical and theoretical density")

# QQ plot du modèle :

plot(fit2, type = "qq", main = "QQ-plot")

```



Le modèle théorique est très éloigné du modèle empirique, et cela est clair sur qq-plot ainsi que sur le graph de la densité. En revanche, le modèle obtenu par approche block maxima a un AIC plus grand que celui-là.


# Tâche 4

Nous soupçonions donc que notre variable soit celle d'une température *(cf Tâche 1)*. Nous soulignons par ailleurs les faits suivants :

- Moyenne à 25°C et médiane à 26°C
- Aucune température sous 6°C
- 3/4 des températures au-dessus de 22°C

Nous hésitions donc entre la température moyenne à Riyad en Arabie Saoudite, ou la température maximale à Madrid en Espagne. Cependant, en considérant également que :

- Saisonnalité vers l'été, un peu avant
- La variable vaut déjà 25 à 30°C  en mai *(cf Tâche 1, tracé sur l'année 2018)*

Nous excluons alors la possibilité de la température maximale de Madrid, car même s'il s'agit d'une région bien plus chaude que la France, un mois de mai à 25/30°C nous paraît bien élevé.
Nous en concluons donc que var2 correspond à la température moyenne à Riyad, Arabie Saoudite.

# Tâche 5

````{r}

class(data2_df$date)
#transform to appropriate time class:
data2_df$date = as.POSIXlt(data2_df$date)
years = 1900 + data2_df$date$year
months = data2_df$date$mon+1
table(months)
data2_df$years = years
data2_df$months = months

chiplot(cbind(data2_df$var3, data2_df$var4), qlim = c(0.8, .9995), xlim = c(0.8, .9995), which = 1)


````

On constate bien que la tail correlation diminue très légèrement lorsque le palier quantile u augmente, et reste bien différent de 0 pour des quantiles élevés. On peut donc supposer qu'il n'y a pas de dépendance asymptotique.

```` {r}

M1 = aggregate(data2_df$var3, by = list(year = data2_df$years), FUN = max)$x
M2 = aggregate(data2_df$var4, by = list(year = data2_df$years), FUN = max)$x

omega = 0:26/26 # define values where to evaluate A
pickands = abvnonpar(omega, data = cbind(M1,M2), method = "cfg", plot = TRUE) 
````

On constate que les deux variables ne sont ni 100% indépendantes, ni 100% dépendantes. Cependant, elles paraissent plus indépendantes que l'inverse. On peut en déduire que si elles s'influencent entre elles, elles ne sont pas entièrement influentes sur l'autre.

Maintenant nous allons estimer plusieurs modèles max-stables (Huesler-Reiss et logistic) et les comparer.

````{r}

fit1 = fbvevd(cbind(M1,M2), model = "log")
fit1

fit2 = fbvevd(cbind(M1,M2), model = "hr")
fit2

````
La différence d'AIC est très légère, mais elle laisse malgré tout voir que que le modèle max-stable d'Huesler-Reiss est plus adapté.

=======
---
title: "Projet sujet 2"
author: "Omar Himych, Morgane Roy"
date: "03/03/2022"
output:
  html_document:
    toc: yes
    toc_depth: 4
    number_sections: yes
---

```{r setup, include = FALSE}
# Setup of options for the rmarkdown document
knitr::opts_chunk$set(echo = TRUE, # By default, codes are shown
                      warning = FALSE, # Warnings are not printed
                      message = FALSE, # Messages are not printed
                      cache = TRUE,
                      out.width = "90%") # Codes results are cached into a 
# dedicated _cache directory which should be ignored by git (and emptied regularly)
```

```{r work, include =FALSE, cache=FALSE}
# set environment
WORK = "C:/Users/rachid/Desktop/Projet_Option/Dossier R/ValeursExtremes"  #à modifier en local pour chacun
setwd(WORK)

library(dplyr)
library(ggplot2)
library(lubridate)
library(extRemes)
library(evd)

data <- load("data.RData")

data_df <<- select(data_df, "date", "var2") 
data2_df <<- select(data2_df, "date", "var3", "var4")

data_df_plot <- data_df

class(data_df$DATE)
#transform to appropriate time class:
data_df$date = as.POSIXlt(data_df$date)
years = 1900 + data_df$date$year
months = data_df$date$mon+1
table(months)
data_df$years = years
data_df$months = months

```
# Introduction

Dans le cadre du projet de valeurs extrêmes, nous avons eu le projet N°2. Pour les tâches 1 à 5 nous avons donc travaillé avec var2 pour *data_df*, et pour la suite, nous avons travaillé avec var3 et var4 pour *data2_df*.

***

# Tâche 1

Pour commencer, voici d'abord un résumé de nos données ainsi qu'un tracé temporel de toutes les données.
```{r warning=FALSE, fig.align='center'}

summary(data_df)

ggplot(data = data_df_plot,
       mapping = aes(x = date, y = var2)) + geom_line() + labs(
  title    = "Évolution temporelle de la variable étudiée",
  subtitle = "Tracé sur la totalité des dates fournies, début des mesures en 1971",
  x        = "Date",
  y        = "Variable")

# format date données : YYYY-MM-JJ

date_debut <- as.Date("2010-01-01","%Y-%m-%j")
date_fin <- as.Date("2019-12-31", "%Y-%m-%j")

ggplot(data = data_df_plot %>% filter(between(as.Date(date,origin="1970-01-01"), date_debut,date_fin)),
       mapping = aes(x = date, y = var2)) + geom_line() + labs(
  title    = "Évolution temporelle de la variable étudiée",
  subtitle = "Période restreinte aux années 2010",
  x        = "Date",
  y        = "Variable")

date_debut <- as.Date("2018-01-01","%Y-%m-%j")
date_fin <- as.Date("2018-12-31", "%Y-%m-%j")
ggplot(data = data_df_plot %>% filter(between(as.Date(date,origin="1970-01-01"), date_debut,date_fin)),
       mapping = aes(x = date, y = var2)) + geom_line()  + labs(
  title    = "Évolution temporelle de la variable étudiée",
  subtitle = "Tracé sur l'année 2018",
  x        = "Date",
  y        = "Variable")

hist(data_df$var2, breaks = 15, main = "Histogramme de la variable", xlab = "var2", ylab="Fréquence")

acf(data_df$var2, na.action = na.pass, xlab = "Lag (jours)", ylab="Auto-corrélation temporelle de var2", main="Auto-corrélation temporelle de la variable en fonction du lag")


```

Nous pouvons constater qu'il s'agit d'une variable avec une certaine saisonnalité, et qui semble atteindre ces valeurs les plus extrêmes en été. Tout cela, en plus de l'échelle de valeurs (de ~7 à ~37) laisse penser à une température. Il s'agit également d'une variable ayant une majorité de valeurs vers 25/26, donc plutôt une région chaude, et qui est auto-corrélée dans le temps de manière assez significative.

# Tâche 2

```` {r, fig.align='center'}

extract_function = function(vec, nr_of_na_allowed = 30){
  if(sum(is.na(vec)) > nr_of_na_allowed){
    return(NA)
  }else{
    return(max(vec, na.rm = TRUE))
  }
}


tmp = aggregate(data_df$var2, by = list(year = data_df$years), FUN = extract_function)
mean(is.na(tmp$x)) # proportion of years with too many NA data
# observations of maxima
obs = tmp$x

ggplot(data = tmp,
       mapping = aes(x = year, y = x)) + geom_line()  + labs(
  title    = "Maxima annuel sur les années avec moins de 30 NA",
  x        = "Year",
  y        = "Maximum")


````

On constate une hausse des maxima annuels sur les années avec moins de 30 données NA.
Faisons maintenant l'estimation du modèle GEV stationnaire puis du modèle non-stationnaire.

```` {r, fig.align='center', warning=FALSE}

fit = fevd(na.omit(obs), type = "GEV", method = "MLE", period.basis = "year", use.phi = TRUE)
summary(fit)
plot(fit, type="density", main="")

fit_nonstationnaire1 = fevd(na.omit(obs), data = data.frame(year = na.omit(tmp)$year), scale.fun = ~1+year, location.fun = ~1+year, type = "GEV", method = "MLE", period.basis = "year", use.phi = TRUE)
summary(fit_nonstationnaire1) 
plot(fit_nonstationnaire1, type="density", main="")

````

Le modèle non-stationnaire a un output d'AIC et de BIC plus faibles, ce qui indique qu'il s'agit d'un modèle qui correspond mieux. On le voit également avec le graphique de densité, où le modèle correspond beaucoup mieux aux données dans le cas du modèle non-stationnaire.

Comme nous avons pu constater que les maxima annuels ont récemment augmenté, ce qui peut être expliqué par le changement climatique dans le cas d'une températuer - notre supposition pour notre variable, il n'est pas étonnant que le modèle non-stationnaire soit plus correct.


Calculons maintenant le 100-year return pour le modèle stationnaire et le modèle non-stationnaire pour 2020.
````{r, eval=FALSE}
# Estimate a 100-year return level for var 2 for the year 2020. 

T_2020 = 366 # in 100 years, we have T days in January


covar1 = cos(1:366/366*2*pi)
covar2 = sin(1:366/366*2*pi)

# For the shape parameter, we simply use the average of estimates for the month of January:
sigma_jan = mean(exp(sigma0 + covar1[1:31] + sigma2 * covar2[1:31]))
sigma_jan
xi = fit$results$par[4]
xi
names(xi) = NULL
u = u_by_month[1]  #threshold in January month

# By construction of the monthly thresholds above, the exceedance probability is as follows:
p_u = 1 - 0.975
x_T = u + sigma_jan * ((T_january*p_u)^xi-1)/xi
x_T 


plot(fit_nonstationnaire1, type="rl",main="", rperiods=100)


````

````{r}


````
# Tâche 3

```` {r}

````

# Tâche 4

Nous soupçonions donc que notre variable soit celle d'une température *(cf Tâche 1)*. Nous soulignons par ailleurs les faits suivants :

- Moyenne à 25°C et médiane à 26°C
- Aucune température sous 6°C
- 3/4 des températures au-dessus de 22°C

Nous hésitions donc entre la température moyenne à Riyad en Arabie Saoudite, ou la température maximale à Madrid en Espagne. Cependant, en considérant également que :

- Saisonnalité vers l'été, un peu avant
- La variable vaut déjà 25 à 30°C  en mai *(cf Tâche 1, tracé sur l'année 2018)*

Nous excluons alors la possibilité de la température maximale de Madrid, car même s'il s'agit d'une région bien plus chaude que la France, un mois de mai à 25/30°C nous paraît bien élevé.
Nous en concluons donc que var2 correspond à la température moyenne à Riyad, Arabie Saoudite.

# Tâche 5

````{r}

class(data2_df$date)
#transform to appropriate time class:
data2_df$date = as.POSIXlt(data2_df$date)
years = 1900 + data2_df$date$year
months = data2_df$date$mon+1
table(months)
data2_df$years = years
data2_df$months = months

chiplot(cbind(data2_df$var3, data2_df$var4), qlim = c(0.8, .9995), xlim = c(0.8, .9995), which = 1)


````

On constate bien que la tail correlation diminue très légèrement lorsque le palier quantile u augmente, et reste bien différent de 0 pour des quantiles élevés. On peut donc supposer qu'il n'y a pas de dépendance asymptotique.

```` {r}

M1 = aggregate(data2_df$var3, by = list(year = data2_df$years), FUN = max)$x
M2 = aggregate(data2_df$var4, by = list(year = data2_df$years), FUN = max)$x

omega = 0:26/26 # define values where to evaluate A
pickands = abvnonpar(omega, data = cbind(M1,M2), method = "cfg", plot = TRUE) 
````

On constate que les deux variables ne sont ni 100% indépendantes, ni 100% dépendantes. Cependant, elles paraissent plus indépendantes que l'inverse. On peut en déduire que si elles s'influencent entre elles, elles ne sont pas entièrement influentes sur l'autre.

Maintenant nous allons estimer plusieurs modèles max-stables (Huesler-Reiss et logistic) et les comparer.

````{r}

fit1 = fbvevd(cbind(M1,M2), model = "log")
fit1

fit2 = fbvevd(cbind(M1,M2), model = "hr")
fit2

````
La différence d'AIC est très légère, mais elle laisse malgré tout voir que que le modèle max-stable d'Huesler-Reiss est plus adapté.

>>>>>>> Stashed changes
